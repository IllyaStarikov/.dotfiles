#!/usr/bin/env zsh
# Omni-Test Suite - Comprehensive Test Runner for Dotfiles
# Runs all test categories with extensive logging and reporting

set -euo pipefail

readonly SCRIPT_NAME="$(basename "$0")"
readonly SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
readonly TEST_DIR="$SCRIPT_DIR"
readonly DOTFILES_DIR="$(dirname "$TEST_DIR")"
readonly TIMESTAMP="$(date +%Y%m%d_%H%M%S)"
readonly REPORT_DIR="${TEST_DIR}/reports/omni_${TIMESTAMP}"

# Test size configuration
TEST_SIZE="${1:-large}"
DEBUG="${DEBUG:-0}"
PARALLEL="${PARALLEL:-1}"

# Colors (use regular variables, not readonly, to avoid conflicts)
if [[ -t 1 ]]; then
    RED=$'\033[0;31m'
    GREEN=$'\033[0;32m'
    YELLOW=$'\033[0;33m'
    BLUE=$'\033[0;34m'
    MAGENTA=$'\033[0;35m'
    CYAN=$'\033[0;36m'
    BOLD=$'\033[1m'
    DIM=$'\033[2m'
    NC=$'\033[0m'
else
    RED="" GREEN="" YELLOW="" BLUE="" MAGENTA="" CYAN="" BOLD="" DIM="" NC=""
fi

# Test categories based on size
declare -A TEST_CATEGORIES
TEST_CATEGORIES[small]="unit sanity smoke"
TEST_CATEGORIES[medium]="unit sanity smoke functional integration configuration acceptance"
TEST_CATEGORIES[large]="unit sanity smoke functional integration configuration acceptance system regression performance e2e security compatibility stress load reliability recovery installation snapshot fuzz mutation chaos"

# Test results tracking
declare -g TOTAL_TESTS=0
declare -g PASSED_TESTS=0
declare -g FAILED_TESTS=0
declare -g SKIPPED_TESTS=0
declare -g TOTAL_TIME=0
declare -A CATEGORY_RESULTS

#######################################
# Display usage information
#######################################
usage() {
    cat << EOF
${BOLD}Omni-Test Suite - Comprehensive Dotfiles Testing${NC}

${BOLD}USAGE:${NC}
    $SCRIPT_NAME [SIZE] [OPTIONS]

${BOLD}SIZE:${NC}
    small    Unit tests and quick checks (< 30s)
    medium   Integration and functional tests (< 5min)
    large    All tests including performance/E2E (default)

${BOLD}OPTIONS:${NC}
    --debug        Enable verbose debug output
    --sequential   Run tests sequentially (default: parallel)
    --report       Generate detailed HTML report
    --help         Show this help message

${BOLD}TEST CATEGORIES:${NC}
    ${CYAN}Functional Testing:${NC}
    - unit         Configuration validation
    - integration  Component interactions
    - system       Full setup verification
    - acceptance   User workflow validation
    - smoke        Critical functionality
    - sanity       Basic operations
    - regression   Bug prevention
    - e2e          End-to-end workflows

    ${CYAN}Non-Functional Testing:${NC}
    - performance  Speed and resource usage
    - load         Resource usage under load
    - stress       Edge case handling
    - volume       Large data handling
    - scalability  Growth capacity
    - security     Security validation
    - compatibility Environment compatibility
    - reliability  Fault tolerance
    - recovery     Error recovery

    ${CYAN}Specialized Testing:${NC}
    - configuration Config file validation
    - installation Setup script testing
    - accessibility Usability testing
    - snapshot     State validation
    - fuzz         Random input testing
    - mutation     Code mutation testing
    - chaos        Resilience testing

${BOLD}EXAMPLES:${NC}
    $SCRIPT_NAME              # Run large test suite
    $SCRIPT_NAME small        # Quick unit tests
    $SCRIPT_NAME medium       # Integration tests
    $SCRIPT_NAME large --debug # Full suite with debugging

EOF
}

#######################################
# Initialize test environment
#######################################
init_test_env() {
    # Create report directory
    mkdir -p "$REPORT_DIR"/{logs,results,metrics}

    # Create and export TEST_TMP_DIR for test helpers
    export TEST_TMP_DIR=$(mktemp -d -t omni-test.XXXXXX)
    export DOTFILES_DIR

    # Initialize main log
    exec 3>&1 4>&2  # Save stdout and stderr
    if [[ "$DEBUG" -eq 1 ]]; then
        exec 1> >(tee -a "$REPORT_DIR/omni-test.log")
        exec 2> >(tee -a "$REPORT_DIR/omni-test-error.log" >&2)
    else
        exec 1>"$REPORT_DIR/omni-test.log"
        exec 2>"$REPORT_DIR/omni-test-error.log"
    fi

    # Source test helpers
    source "$TEST_DIR/lib/test_helpers.zsh"

    # Create test summary file
    cat > "$REPORT_DIR/summary.json" << EOF
{
    "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
    "test_size": "$TEST_SIZE",
    "dotfiles_dir": "$DOTFILES_DIR",
    "system": {
        "os": "$(uname -s)",
        "arch": "$(uname -m)",
        "hostname": "$(hostname)",
        "user": "$(whoami)"
    },
    "tests": {}
}
EOF
}

#######################################
# Log with timestamp and level
#######################################
log() {
    local level="$1"
    shift
    local message="$*"
    local timestamp="$(date '+%H:%M:%S')"

    case "$level" in
        DEBUG)
            [[ "$DEBUG" -eq 1 ]] && echo "${DIM}[$timestamp]${NC} ${CYAN}[DEBUG]${NC} $message" >&3
            ;;
        INFO)
            echo "${DIM}[$timestamp]${NC} ${BLUE}[INFO]${NC} $message" >&3
            ;;
        SUCCESS)
            echo "${DIM}[$timestamp]${NC} ${GREEN}[✓]${NC} $message" >&3
            ;;
        WARN)
            echo "${DIM}[$timestamp]${NC} ${YELLOW}[⚠]${NC} $message" >&3
            ;;
        ERROR)
            echo "${DIM}[$timestamp]${NC} ${RED}[✗]${NC} $message" >&3
            ;;
        PROGRESS)
            echo "${DIM}[$timestamp]${NC} ${MAGENTA}[▶]${NC} $message" >&3
            ;;
    esac

    # Also log to file
    echo "[$timestamp] [$level] $message" >> "$REPORT_DIR/omni-test.log"
}

#######################################
# Run test category
#######################################
run_test_category() {
    local category="$1"
    local category_dir="$TEST_DIR/$category"
    local start_time=$(date +%s)

    log PROGRESS "Running $category tests..."

    # Initialize category results
    CATEGORY_RESULTS[$category]="0:0:0"  # passed:failed:skipped

    if [[ ! -d "$category_dir" ]]; then
        log WARN "Category '$category' directory not found, skipping"
        return 0
    fi

    # Find and run all test scripts in category
    local test_files=()
    while IFS= read -r -d '' file; do
        test_files+=("$file")
    done < <(find "$category_dir" -name "*test*.sh" -type f -print0 2>/dev/null)

    if [[ ${#test_files[@]} -eq 0 ]]; then
        log INFO "No tests found in $category"
        return 0
    fi

    local category_passed=0
    local category_failed=0
    local category_skipped=0

    for test_file in "${test_files[@]}"; do
        if [[ ! -x "$test_file" ]]; then
            chmod +x "$test_file"
        fi

        local test_name="$(basename "$test_file" .sh)"
        local test_log="$REPORT_DIR/logs/${category}_${test_name}.log"

        log DEBUG "Executing: $test_file"

        # Run test with timeout
        local timeout_duration=300  # 5 minutes default
        case "$category" in
            performance|e2e|stress|load)
                timeout_duration=600  # 10 minutes for heavy tests
                ;;
            unit|smoke|sanity)
                timeout_duration=60   # 1 minute for quick tests
                ;;
        esac

        if timeout "$timeout_duration" "$test_file" > "$test_log" 2>&1; then
            log SUCCESS "$test_name passed"
            ((category_passed++))
            ((PASSED_TESTS++))
        else
            local exit_code=$?
            if [[ $exit_code -eq 124 ]]; then
                log ERROR "$test_name timed out after ${timeout_duration}s"
                ((category_failed++))
                ((FAILED_TESTS++))
            elif [[ $exit_code -eq 77 ]]; then  # Convention for skipped tests
                log INFO "$test_name skipped"
                ((category_skipped++))
                ((SKIPPED_TESTS++))
            else
                log ERROR "$test_name failed with exit code $exit_code"
                ((category_failed++))
                ((FAILED_TESTS++))

                if [[ "$DEBUG" -eq 1 ]]; then
                    echo "${DIM}--- Test output ---${NC}" >&3
                    tail -20 "$test_log" >&3
                    echo "${DIM}--- End output ---${NC}" >&3
                fi
            fi
        fi
        ((TOTAL_TESTS++))
    done

    CATEGORY_RESULTS[$category]="$category_passed:$category_failed:$category_skipped"

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    log INFO "Category $category completed in ${duration}s (Passed: $category_passed, Failed: $category_failed, Skipped: $category_skipped)"

    return 0
}

#######################################
# Run tests in parallel
#######################################
run_tests_parallel() {
    local categories=($@)
    local pids=()

    for category in "${categories[@]}"; do
        run_test_category "$category" &
        pids+=($!)
    done

    # Wait for all background jobs
    for pid in "${pids[@]}"; do
        wait "$pid"
    done
}

#######################################
# Generate test report
#######################################
generate_report() {
    log PROGRESS "Generating test report..."

    local report_file="$REPORT_DIR/report.html"

    cat > "$report_file" << 'HTML'
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Omni-Test Report</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
        h2 { color: #34495e; margin-top: 30px; }
        .summary {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .stat {
            background: #fff;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
            border: 2px solid #e0e0e0;
        }
        .stat.passed { border-color: #27ae60; }
        .stat.failed { border-color: #e74c3c; }
        .stat.skipped { border-color: #f39c12; }
        .stat .number { font-size: 2em; font-weight: bold; }
        .stat .label { color: #7f8c8d; text-transform: uppercase; font-size: 0.9em; }
        .category {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        .category h3 { margin-top: 0; color: #2c3e50; }
        .progress-bar {
            height: 20px;
            background: #ecf0f1;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        .progress-bar .passed { background: #27ae60; }
        .progress-bar .failed { background: #e74c3c; }
        .progress-bar .skipped { background: #f39c12; }
        .progress-bar > div {
            height: 100%;
            float: left;
            transition: width 0.3s;
        }
        .timestamp { color: #95a5a6; font-size: 0.9em; }
        .footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e0e0e0;
            color: #7f8c8d;
        }
    </style>
</head>
<body>
    <h1>🧪 Omni-Test Report</h1>
HTML

    # Add timestamp and configuration
    echo "    <div class='summary'>" >> "$report_file"
    echo "        <p class='timestamp'>Generated: $(date)</p>" >> "$report_file"
    echo "        <p>Test Size: <strong>$TEST_SIZE</strong></p>" >> "$report_file"
    echo "        <p>System: $(uname -s) $(uname -m)</p>" >> "$report_file"
    echo "    </div>" >> "$report_file"

    # Add statistics
    local pass_rate=0
    if [[ $TOTAL_TESTS -gt 0 ]]; then
        pass_rate=$(( (PASSED_TESTS * 100) / TOTAL_TESTS ))
    fi

    echo "    <div class='stats'>" >> "$report_file"
    echo "        <div class='stat passed'>" >> "$report_file"
    echo "            <div class='number'>$PASSED_TESTS</div>" >> "$report_file"
    echo "            <div class='label'>Passed</div>" >> "$report_file"
    echo "        </div>" >> "$report_file"
    echo "        <div class='stat failed'>" >> "$report_file"
    echo "            <div class='number'>$FAILED_TESTS</div>" >> "$report_file"
    echo "            <div class='label'>Failed</div>" >> "$report_file"
    echo "        </div>" >> "$report_file"
    echo "        <div class='stat skipped'>" >> "$report_file"
    echo "            <div class='number'>$SKIPPED_TESTS</div>" >> "$report_file"
    echo "            <div class='label'>Skipped</div>" >> "$report_file"
    echo "        </div>" >> "$report_file"
    echo "        <div class='stat'>" >> "$report_file"
    echo "            <div class='number'>$pass_rate%</div>" >> "$report_file"
    echo "            <div class='label'>Pass Rate</div>" >> "$report_file"
    echo "        </div>" >> "$report_file"
    echo "    </div>" >> "$report_file"

    # Add category results
    echo "    <h2>Test Categories</h2>" >> "$report_file"

    for category in "${!CATEGORY_RESULTS[@]}"; do
        IFS=: read -r passed failed skipped <<< "${CATEGORY_RESULTS[$category]}"
        local total=$((passed + failed + skipped))

        if [[ $total -eq 0 ]]; then
            continue
        fi

        local pass_pct=$(( (passed * 100) / total ))
        local fail_pct=$(( (failed * 100) / total ))
        local skip_pct=$(( (skipped * 100) / total ))

        echo "    <div class='category'>" >> "$report_file"
        echo "        <h3>$category</h3>" >> "$report_file"
        echo "        <p>Total: $total tests</p>" >> "$report_file"
        echo "        <div class='progress-bar'>" >> "$report_file"
        [[ $passed -gt 0 ]] && echo "            <div class='passed' style='width: ${pass_pct}%'></div>" >> "$report_file"
        [[ $failed -gt 0 ]] && echo "            <div class='failed' style='width: ${fail_pct}%'></div>" >> "$report_file"
        [[ $skipped -gt 0 ]] && echo "            <div class='skipped' style='width: ${skip_pct}%'></div>" >> "$report_file"
        echo "        </div>" >> "$report_file"
        echo "        <p>Passed: $passed | Failed: $failed | Skipped: $skipped</p>" >> "$report_file"
        echo "    </div>" >> "$report_file"
    done

    # Add footer
    echo "    <div class='footer'>" >> "$report_file"
    echo "        <p>Dotfiles Test Suite v5.0 | Report: $REPORT_DIR</p>" >> "$report_file"
    echo "    </div>" >> "$report_file"
    echo "</body>" >> "$report_file"
    echo "</html>" >> "$report_file"

    log SUCCESS "Report generated: $report_file"

    # Also generate JSON report for programmatic access
    local json_report="$REPORT_DIR/report.json"
    {
        echo "{"
        echo '  "summary": {'
        echo "    \"total\": $TOTAL_TESTS,"
        echo "    \"passed\": $PASSED_TESTS,"
        echo "    \"failed\": $FAILED_TESTS,"
        echo "    \"skipped\": $SKIPPED_TESTS,"
        echo "    \"pass_rate\": $pass_rate"
        echo "  },"
        echo '  "categories": {'

        local first=1
        for category in "${!CATEGORY_RESULTS[@]}"; do
            [[ $first -eq 0 ]] && echo ","
            first=0

            IFS=: read -r passed failed skipped <<< "${CATEGORY_RESULTS[$category]}"
            echo -n "    \"$category\": {\"passed\": $passed, \"failed\": $failed, \"skipped\": $skipped}"
        done

        echo ""
        echo "  }"
        echo "}"
    } > "$json_report"
}

#######################################
# Print summary
#######################################
print_summary() {
    # Restore original stdout/stderr
    exec 1>&3 2>&4

    echo
    echo "${BOLD}${BLUE}════════════════════════════════════════════════════════${NC}"
    echo "${BOLD}                    TEST SUMMARY                        ${NC}"
    echo "${BOLD}${BLUE}════════════════════════════════════════════════════════${NC}"
    echo

    # Print stats
    local pass_rate=0
    if [[ $TOTAL_TESTS -gt 0 ]]; then
        pass_rate=$(( (PASSED_TESTS * 100) / TOTAL_TESTS ))
    fi

    echo "  Test Size:    ${BOLD}$TEST_SIZE${NC}"
    echo "  Total Tests:  ${BOLD}$TOTAL_TESTS${NC}"
    echo
    echo "  ${GREEN}Passed:${NC}      $PASSED_TESTS"
    echo "  ${RED}Failed:${NC}      $FAILED_TESTS"
    echo "  ${YELLOW}Skipped:${NC}     $SKIPPED_TESTS"
    echo
    echo "  Pass Rate:    ${BOLD}$pass_rate%${NC}"
    echo

    # Print category summary
    if [[ ${#CATEGORY_RESULTS[@]} -gt 0 ]]; then
        echo "${BOLD}Category Results:${NC}"
        for category in "${!CATEGORY_RESULTS[@]}"; do
            IFS=: read -r passed failed skipped <<< "${CATEGORY_RESULTS[$category]}"
            local total=$((passed + failed + skipped))

            if [[ $total -gt 0 ]]; then
                printf "  %-20s " "$category:"

                if [[ $failed -eq 0 ]]; then
                    echo "${GREEN}✓${NC} ($passed/$total)"
                elif [[ $passed -eq 0 ]]; then
                    echo "${RED}✗${NC} ($failed/$total)"
                else
                    echo "${YELLOW}⚠${NC} ($passed passed, $failed failed, $skipped skipped)"
                fi
            fi
        done | sort
        echo
    fi

    echo "  Report:       ${CYAN}$REPORT_DIR${NC}"
    echo

    # Return appropriate exit code
    if [[ $FAILED_TESTS -gt 0 ]]; then
        echo "${RED}${BOLD}TESTS FAILED${NC}"
        return 1
    else
        echo "${GREEN}${BOLD}ALL TESTS PASSED${NC}"
        return 0
    fi
}

#######################################
# Main execution
#######################################
main() {
    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --help|-h)
                usage
                exit 0
                ;;
            --debug)
                DEBUG=1
                shift
                ;;
            --sequential)
                PARALLEL=0
                shift
                ;;
            --report)
                GENERATE_REPORT=1
                shift
                ;;
            small|medium|large)
                TEST_SIZE="$1"
                shift
                ;;
            *)
                echo "Unknown option: $1"
                usage
                exit 1
                ;;
        esac
    done

    # Initialize
    init_test_env

    # Display header
    echo "${BOLD}${CYAN}╔════════════════════════════════════════════════════════╗${NC}"
    echo "${BOLD}${CYAN}║              OMNI-TEST SUITE v5.0                      ║${NC}"
    echo "${BOLD}${CYAN}╚════════════════════════════════════════════════════════╝${NC}"
    echo
    echo "Test Size: ${BOLD}$TEST_SIZE${NC}"
    echo "Categories: ${TEST_CATEGORIES[$TEST_SIZE]}"
    echo "Debug Mode: $([[ $DEBUG -eq 1 ]] && echo "Enabled" || echo "Disabled")"
    echo "Parallel: $([[ $PARALLEL -eq 1 ]] && echo "Enabled" || echo "Disabled")"
    echo

    # Get test categories for selected size
    local categories=(${=TEST_CATEGORIES[$TEST_SIZE]})

    log INFO "Starting $TEST_SIZE test suite with ${#categories[@]} categories"

    # Run tests
    local start_time=$(date +%s)

    if [[ $PARALLEL -eq 1 ]]; then
        run_tests_parallel "${categories[@]}"
    else
        for category in "${categories[@]}"; do
            run_test_category "$category"
        done
    fi

    local end_time=$(date +%s)
    TOTAL_TIME=$((end_time - start_time))

    log INFO "All tests completed in ${TOTAL_TIME} seconds"

    # Generate report
    generate_report

    # Print summary
    print_summary
}

# Cleanup function
cleanup() {
    [[ -n "${TEST_TMP_DIR:-}" ]] && [[ -d "${TEST_TMP_DIR}" ]] && rm -rf "${TEST_TMP_DIR}"
}

# Set up cleanup trap
trap cleanup EXIT

# Run main function
main "$@"